{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis has been using as a tool to cassify response from your user/customer as 'positive' or 'negative' or even 'neutral'.\n",
    "\n",
    "Before use of sentiment analysis algorithms, people were trying to evaluate those user response based on simple practices like : extracting keywords from the content and restricting their findings only on 'what people are talking about?'.\n",
    "This will never helps you out to answer few important questions 'What people are feeling and thinking about your product?'. That means only the explicit statements/reviews or opinions will have measurable output with your old approach. What would you do with large number of implicit comments?\n",
    "\n",
    "Even though, it is extremely arduous to determine the actual tone from given text, we can try to develop a more accurate version of our older one.\n",
    "\n",
    "In the following blog, I am going to help you to understand the basic norms natural language processing in order solve the problem of sentiment analysis. I have divided the whole content into following sub-topics:\n",
    "\t\t1. Problem Statement\n",
    "\t\t2. Problem Analysis\n",
    "\t\t2. Data Collection\n",
    "\t\t3. Model Development\n",
    "\t\t4. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Problem Statement:\n",
    "\n",
    "Though there are lots of challenges out there that can be solved using data science, I come up with a very basic problem. Why I am choosing this -- simply because I do not want to waste my energy just by thinking about big problems and also not to waste your energy to read it and forget in couple of hours :). \n",
    "\n",
    "PROBLEM: We have to classify the sentiment of news titles (Whether positive or negative.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Analysis:\n",
    "\n",
    "For this part, you should be able to answer following questions:\n",
    "\t   1. What will be the input for your program?\n",
    "\t   2. What output you are expecting ?\n",
    "          Whether it is continous value (something like given by regression model?) or   it is a label value (like spam or ham?)\n",
    "\t   3. What sort of problem is this?\n",
    "          Whether it is supervised or unsupervised learning? Is it a problem of classification or  clustering ? \n",
    "\t   4. What are the features you should take into account?\n",
    "\n",
    "\tAs we already know, the sentiment of any text can be classified into\n",
    "    three major categories (positive, negative, and neutral), the problem\n",
    "    is clearly a type of classification. And, we will be using labelled\n",
    "    data (supervised learning).\n",
    "\n",
    "\tYour ML model will not be able to predict the correctly if you don't\n",
    "    have enough training data.\n",
    "\n",
    "\tSuppose, you feed following two texts into your algorithm,\n",
    "\n",
    "\t'New government rule aganist people will' -->Negative\n",
    "\t'Government offers funding opportunities for the startups' -->Positive\n",
    "\n",
    "\tWhat would you expect if the new input text is\n",
    "\n",
    "\t'Tech Companies in Nepal are having problem to raise funds.\n",
    "    Nevertheless, they are doing great with customer acquisition'\n",
    "\n",
    "\tYour model is not being taught with this kind of complex structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collection:\n",
    "\n",
    "After you have your problem analysis, you should only focus couple of your days to gather the related data.\n",
    "\n",
    " a. Training / testing data collection\n",
    "\n",
    "\tI have found labelled data maintained by UCI Machine Learning Repository. The data consists of product reviews and labelled as 1 for positive and 0 for negative responses.\n",
    "\n",
    "\t\thttps://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
    "\n",
    " b. Unseen data collection\n",
    "    Unseen data set is collection of documents wihtout label. We will evaluate our final     model based on unseen data.\n",
    "\tIn my case, I need to collect news titles from the news portals. I chose our national daily paper ( http://www.ekantipur.com/eng) as a data source.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here is the sample code that I have used for scraping news site.\n",
    "# news_spider.py\n",
    "\n",
    "# This module scrolls through news site (ekantipur.com) and collects news titles.\n",
    "import time\n",
    "import csv\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "browser = webdriver.Chrome()\n",
    "\n",
    "browser.get(\"http://www.ekantipur.com/eng\")\n",
    "time.sleep(1)\n",
    "\n",
    "elem = browser.find_element_by_tag_name(\"body\")\n",
    "\n",
    "# set number of pages be scrolled\n",
    "no_of_pagedowns = 5\n",
    "\n",
    "# scroll page (handling infinite page scrolling)\n",
    "while no_of_pagedowns:\n",
    "    elem.send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(0.2)\n",
    "    no_of_pagedowns -= 1\n",
    "\n",
    "content = browser.page_source\n",
    "soup = BeautifulSoup(content, \"html5lib\")\n",
    "browser.close()\n",
    "\n",
    "news_title_containers = soup.find_all(\n",
    "    \"div\", attrs={'class': 'display-news-title'})\n",
    "with open('news_titles.csv', 'w') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for each_container in news_title_containers:\n",
    "        title_link = each_container.find('a')\n",
    "        news_title = title_link.string.strip()\n",
    "        writer.writerow([news_title])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model development\n",
    "\n",
    "Before starting on model development, you should be familiar with few terms that will help you to understand the process:\n",
    "\n",
    "i. Training and Testing\n",
    "\n",
    "This is the process of applying our model on data whose true classes are already known. Once the model is well trained with those data, we need to compute the proportion of the time our classifier will accurately predict the new data. This process is know as testing.\n",
    "\n",
    "ii. Bias(underfitting) and Variance(overfitting)\n",
    "\n",
    "Ideally, whenever we try to develop ML model, we try to capture all the features(information) of the training data and also want our model to generalize the output for unseen input data.\n",
    "\n",
    "<img src=\"files/final_draw.png\">\n",
    "In the diagram above, the linear model is underfit\n",
    " - The model doesn't take care of all the information provided (high bias)\n",
    " - If you provide new data to this model, it will not change it's behavior(shape) (low variance)\n",
    "\n",
    "\n",
    "On the other hand, the model with high degree polynomial at the right is overfit\n",
    " - The model will take into account all the data points (low bias)\n",
    " - The curve will try to change it's shape in order to capture new data points, loosing the generality of the model (high variance)\n",
    "\n",
    "\n",
    "iii. Cross Validation:\n",
    "\n",
    "Most of the time, when we are developing ML model we simply split the data randomly into train and test set and feed into our model. While doing so, we may have a high accuracy on that particular chunk of training data set. What if your model perform differently and give lower accuracy while feeding different chuncks of from the sama data points? So, in cross-validation, we will be creating number of train/test splits and measure the accuracy by calculating average on each of the spits.\n",
    "\n",
    "\n",
    "iv. Learning Curve\n",
    "\t\n",
    "  Learning curve helps to measure the performance of your model based on variations on your training data supply.\n",
    "\n",
    "   Why you need to plot learning curve?\n",
    "   - You can identify the correct spot on your curve where bias and variance is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have divided the whole model development process into following subprocesses:\n",
    "\n",
    " #### A. Loading data / Split into training and test set\n",
    "\n",
    "I have divided the corpus provided by UCI Machine Learning Repository for training and testing as \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "TRAIN_CORPUS_FILES = ['amazon.txt', 'imdb.txt']\n",
    "TEST_CORPUS_FILES = ['yelp.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Feature Extraction\n",
    "  i. We need to convert text data into numerical feature vectors\n",
    "  \n",
    "  Most of the algorithms expect numerical data rather than raw (a sequence of symbols). This process is called vectorization -- turning text documents into numerical representation. This whole process comprises of - tokenization, counting and normalization of data ( More specifically called prcess of Bag of Words). The document will be represented by the occurrences of words in the document rather than the relative position of them.\n",
    "  \n",
    "Let us have a closer look into vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_train_data = ['Dispute delays National Assembly formation process',\n",
    "              'Country is looking to encourage entrepreneurs and startup process',\n",
    "              'Airline fuel surcharges to go up from Tuesday'] \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# instantiate Vectorizer\n",
    "vec = CountVectorizer()\n",
    "\n",
    "# feed/learn the 'vocabulary' of the training data\n",
    "vec.fit(sample_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the definition above, the `CountVectorizer` simply make whole words lowercase, remove duplicates, and remove single word characters as suggested by regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airline',\n",
       " 'and',\n",
       " 'assembly',\n",
       " 'country',\n",
       " 'delays',\n",
       " 'dispute',\n",
       " 'encourage',\n",
       " 'entrepreneurs',\n",
       " 'formation',\n",
       " 'from',\n",
       " 'fuel',\n",
       " 'go',\n",
       " 'is',\n",
       " 'looking',\n",
       " 'national',\n",
       " 'process',\n",
       " 'startup',\n",
       " 'surcharges',\n",
       " 'to',\n",
       " 'tuesday',\n",
       " 'up']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# current fitted vocabulary\n",
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a document-term matrix. This simply describes the occurence of terms collection of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x21 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 23 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform training data into a 'document-term matrix'\n",
    "sample_train_dtm = vec.transform(sample_train_data)\n",
    "sample_train_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the output, we can see that it creates a matrix with 3 rows and 21 columns.\n",
    " - Here, 3 colums because we have three documents all together\n",
    " - And, 21 rows because we have 21 terms (learned during fitting step above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's observe the dense matrix\n",
    "sample_train_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>and</th>\n",
       "      <th>assembly</th>\n",
       "      <th>country</th>\n",
       "      <th>delays</th>\n",
       "      <th>dispute</th>\n",
       "      <th>encourage</th>\n",
       "      <th>entrepreneurs</th>\n",
       "      <th>formation</th>\n",
       "      <th>from</th>\n",
       "      <th>...</th>\n",
       "      <th>go</th>\n",
       "      <th>is</th>\n",
       "      <th>looking</th>\n",
       "      <th>national</th>\n",
       "      <th>process</th>\n",
       "      <th>startup</th>\n",
       "      <th>surcharges</th>\n",
       "      <th>to</th>\n",
       "      <th>tuesday</th>\n",
       "      <th>up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   airline  and  assembly  country  delays  dispute  encourage  entrepreneurs  \\\n",
       "0        0    0         1        0       1        1          0              0   \n",
       "1        0    1         0        1       0        0          1              1   \n",
       "2        1    0         0        0       0        0          0              0   \n",
       "\n",
       "   formation  from ...  go  is  looking  national  process  startup  \\\n",
       "0          1     0 ...   0   0        0         1        1        0   \n",
       "1          0     0 ...   0   1        1         0        1        1   \n",
       "2          0     1 ...   1   0        0         0        0        0   \n",
       "\n",
       "   surcharges  to  tuesday  up  \n",
       "0           0   0        0   0  \n",
       "1           0   1        0   0  \n",
       "2           1   1        1   1  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us see the complete data map using pandas\n",
    "import pandas as pd\n",
    "pd.DataFrame(sample_train_dtm.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that most of the feature values are `ZEROs`. Why is that? Because most of the documents can be represented by small sets of words. Suppose, you have collected almost 100 reviews (documents) from a site. Then, there could be all together 1000 unique words (corpus) and a single review (single document) might have 10 unique words. right?\n",
    "\n",
    "Storing such a big number of `ZEROs` does not make sense. So, scikit-learn internally uses `scipy.sparse` package to store such a matrix in memory and it also speed up the operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let us now test our model with new example \n",
    "sample_test_data = ['Country is looking to encourage agriculture schemes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data into a document-term matrix (using existing vocabulary)\n",
    "sample_test_dtm = vec.transform(sample_test_data)\n",
    "sample_test_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>and</th>\n",
       "      <th>assembly</th>\n",
       "      <th>country</th>\n",
       "      <th>delays</th>\n",
       "      <th>dispute</th>\n",
       "      <th>encourage</th>\n",
       "      <th>entrepreneurs</th>\n",
       "      <th>formation</th>\n",
       "      <th>from</th>\n",
       "      <th>...</th>\n",
       "      <th>go</th>\n",
       "      <th>is</th>\n",
       "      <th>looking</th>\n",
       "      <th>national</th>\n",
       "      <th>process</th>\n",
       "      <th>startup</th>\n",
       "      <th>surcharges</th>\n",
       "      <th>to</th>\n",
       "      <th>tuesday</th>\n",
       "      <th>up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   airline  and  assembly  country  delays  dispute  encourage  entrepreneurs  \\\n",
       "0        0    0         0        1       0        0          1              0   \n",
       "\n",
       "   formation  from ...  go  is  looking  national  process  startup  \\\n",
       "0          0     0 ...   0   1        1         0        0        0   \n",
       "\n",
       "   surcharges  to  tuesday  up  \n",
       "0           0   1        0   0  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the vocabulary and document-term matrix together\n",
    "pd.DataFrame(sample_test_dtm.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that `agriculture` and  `schemes` are not the part of output. For our model, to make prediction about this new observation, this new data must have the same features as the training observations. We did not train on the features \"`agriculture` and  `schemes`\" so our model would not be able to predict. Make any sense ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Problem with `CountVectorizer` **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If are looking at two different documents talking about same topic, one bit longer than\n",
    "other, you will find the average count values in the longer one will be higher than\n",
    "the shorter document. That means, `CountVectorizer` simply gives equal weightage to each\n",
    "of the word present in your document (one word represents one column in DataFrame). And,\n",
    "when you have new document, it simply maps 1 if the word is present in that document\n",
    "otherwise map to 0.\n",
    "\n",
    "So as to avoid this problem, we can use `TfidfTransformer` which evaluates frequencies of \n",
    "words rather than their occurences in the document and helps model to figure out which of\n",
    "the word is more important in the document and also with respect to the corpus.\n",
    "\n",
    "\n",
    "#### Let's  take a closer look at `Tfidf`\n",
    "\n",
    "Suppose you have a corpus of documents talking about `Mountains`. Now, `Tfidf` will work in three different parts\n",
    "\n",
    "#### i. Term Frequency (Tf)\n",
    "  It simply looks for the number of times a particular term occurs in your single document.\n",
    "  Let's take an example. You can find word `is` in almost all of your documents. Do you think the word `is` has any credit to describe your document ? But, at the same time, if you find word `Everest` in the document, you can figure out that the document is about `Mountains`.\n",
    " So, we need to find a way to reduce the weightage of word `is`. One of the method is calculating reciprocal of the count value, which minimizes the significant of word `is` in the document. But, you might face another problem here. Suppose, you encounter a word in your document (let's say `stratovolcano` having rare occurence in your document). If you take reciprocal of this occurence, you will get 1 (or close to 1 - hightest weightage). But, this word does not give much sense about `Montains`.\n",
    " \n",
    "For now, we can keep the word count as follows:\n",
    "\n",
    "    `tf(\"is\") = 1000`\n",
    "    `tf(\"Everest\") = 50`\n",
    "    `tf(\"stratovolcano\") = 2`\n",
    "\n",
    "\n",
    "#### ii. Inverser Document Frequency (iDF)\n",
    "\n",
    "  The problem of occurence of rare and more frequent words will be handled by this method.\n",
    "   Inverser Document Frequency gives downscale weights for words that occur in many documents in the corpus by taking log of number of total documents in your corpus divided by the total number of documents having occurence of the word.\n",
    "   i.e iDF = log (total number of documents/ total documents with word occurence)\n",
    "   \n",
    "   Let's calculate iDF for above words. (Suppose we have total 20 documents)\n",
    "   \n",
    "    `iDF(\"is\") = log(20/20) = 0` , Since 'is' occurs in all the documents\n",
    "    `iDF(\"Everest\") = log(20/5) = 0.6`,  Since corpus is talking about 'Mountains'\n",
    "    `iDF(\"stratovolcano\") = log(10/1) = 1 `, Since `stratovolcano` occurs in one doc.\n",
    "   \n",
    "#### iii. TfiDF\n",
    "    \n",
    "    TfiDF = TF * iDF\n",
    "    \n",
    "    Therefore,\n",
    "        `TfiDF(\"is\") = 1000 * 0 = 0`\n",
    "        `TfiDF(\"Everest\") = 50 * 0.6 = 30`\n",
    "        `TfiDF(\"stratovolcano\") = 2 * 1 = 2`\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calcuate TfiDF for our sample train data above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "sample_train_tfidf = tfidf_vec.fit_transform(sample_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>and</th>\n",
       "      <th>assembly</th>\n",
       "      <th>country</th>\n",
       "      <th>delays</th>\n",
       "      <th>dispute</th>\n",
       "      <th>encourage</th>\n",
       "      <th>entrepreneurs</th>\n",
       "      <th>formation</th>\n",
       "      <th>from</th>\n",
       "      <th>...</th>\n",
       "      <th>go</th>\n",
       "      <th>is</th>\n",
       "      <th>looking</th>\n",
       "      <th>national</th>\n",
       "      <th>process</th>\n",
       "      <th>startup</th>\n",
       "      <th>surcharges</th>\n",
       "      <th>to</th>\n",
       "      <th>tuesday</th>\n",
       "      <th>up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.322002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350139</td>\n",
       "      <td>0.350139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350139</td>\n",
       "      <td>0.350139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266290</td>\n",
       "      <td>0.350139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.363255</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363255</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363255</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363255</td>\n",
       "      <td>0.276265</td>\n",
       "      <td>0.363255</td>\n",
       "      <td>0.363255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    airline       and  assembly   country    delays   dispute  encourage  \\\n",
       "0  0.000000  0.000000  0.423394  0.000000  0.423394  0.423394   0.000000   \n",
       "1  0.000000  0.350139  0.000000  0.350139  0.000000  0.000000   0.350139   \n",
       "2  0.363255  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "\n",
       "   entrepreneurs  formation      from    ...           go        is   looking  \\\n",
       "0       0.000000   0.423394  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "1       0.350139   0.000000  0.000000    ...     0.000000  0.350139  0.350139   \n",
       "2       0.000000   0.000000  0.363255    ...     0.363255  0.000000  0.000000   \n",
       "\n",
       "   national   process   startup  surcharges        to   tuesday        up  \n",
       "0  0.423394  0.322002  0.000000    0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.000000  0.266290  0.350139    0.000000  0.266290  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.000000    0.363255  0.276265  0.363255  0.363255  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(sample_train_tfidf.toarray(), columns=tfidf_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Choose estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using `SVM` as my estimator. We will not talk details on SVM because it might take long hours to understand the algorithms.\n",
    "In brief, `linear SVM` tries to find an optimal vector ( a line) sperating two data points i.e classifying data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# news_classifier.py\n",
    "import pickle\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "\n",
    "TRAIN_CORPUS_FILES = ['amazon.txt', 'imdb.txt']\n",
    "TEST_CORPUS_FILES = ['yelp.txt']\n",
    "\n",
    "class NewsClassification:\n",
    "    def __init__(self):\n",
    "        self.train_data = []\n",
    "        self.train_labels = []\n",
    "        self.test_data = []\n",
    "        self.test_labels = []\n",
    "\n",
    "        self.train_vectors = None\n",
    "        self.test_vectors = None\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.model = None\n",
    "\n",
    "    def read_news_corpus(self, file):\n",
    "        with open('news_corpus/%s' % (file), 'r') as f:\n",
    "            return f.readlines()\n",
    "\n",
    "    def prepare_data(self, prepared_for='training'):\n",
    "        data = []\n",
    "        labels = []\n",
    "        if prepared_for.strip().lower() == 'training':\n",
    "            CORPUS_FILES = TRAIN_CORPUS_FILES\n",
    "        else:\n",
    "            CORPUS_FILES = TEST_CORPUS_FILES\n",
    "\n",
    "        for train_file in CORPUS_FILES:\n",
    "            rows = self.read_news_corpus(train_file)\n",
    "            for row in rows:\n",
    "                line = row.strip().rsplit('\\t', 1)\n",
    "                try:\n",
    "                    text = line[0]\n",
    "                    label = line[1].replace('\\t', '').strip()\n",
    "                    data.append(text)\n",
    "                    labels.append(label)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        return data, labels\n",
    "\n",
    "    def prepare_feature_vectors(self):\n",
    "        # Train the feature vectors\n",
    "        self.train_vectors = self.vectorizer.fit_transform(self.train_data)\n",
    "        # Apply model on test data\n",
    "        # : since they have already been fit to the training set\n",
    "        self.test_vectors = self.vectorizer.transform(self.test_data)\n",
    "\n",
    "    def prepare_model(self, classifier_type='svm', kernel='linear'):\n",
    "        self.model = svm.SVC(kernel=kernel)\n",
    "        self.model.fit(self.train_vectors, self.train_labels)\n",
    "\n",
    "        # let's save our model using pickle.\n",
    "        # we will using this model later for unseen data as well.\n",
    "        data_struct = {'vectorizer': self.vectorizer, 'model': self.model}\n",
    "        with open('%s.bin' % kernel, 'wb') as f:\n",
    "            pickle.dump(data_struct, f)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    news_classifier = NewsClassification()\n",
    "    news_classifier.train_data, news_classifier.train_labels =\\\n",
    "        news_classifier.prepare_data()\n",
    "    news_classifier.test_data, news_classifier.test_labels =\\\n",
    "        news_classifier.prepare_data('testing')\n",
    "\n",
    "    news_classifier.prepare_feature_vectors()\n",
    "    news_classifier.prepare_model(kernel='linear')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. Prediction with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                title actual_prediction predicted_value\n",
      "0                            Wow... Loved this place.                 1               1\n",
      "1                                  Crust is not good.                 0               0\n",
      "2           Not tasty and the texture was just nasty.                 0               0\n",
      "3   Stopped by during the late May bank holiday of...                 1               1\n",
      "4   The selection on the menu was great and so wer...                 1               1\n",
      "5      Now I am getting angry and I want my damn pho.                 0               1\n",
      "6               Honeslty it didn't taste THAT fresh.)                 0               0\n",
      "7   The potatoes were like rubber and you could te...                 0               0\n",
      "8                           The fries were great too.                 1               1\n",
      "9                                      A great touch.                 1               1\n",
      "10                           Service was very prompt.                 1               1\n",
      "11                                 Would not go back.                 0               0\n",
      "12  The cashier had no care what so ever on what I...                 0               1\n",
      "13  I tried the Cape Cod ravoli, chicken,with cran...                 1               1\n",
      "14  I was disgusted because I was pretty sure that...                 0               0\n"
     ]
    }
   ],
   "source": [
    "# let's run prediction with our test data\n",
    "prediction = news_classifier.model.predict(news_classifier.test_vectors)\n",
    "\n",
    "# create a list of final data\n",
    "with open('news_corpus/yelp.txt', 'r') as f:\n",
    "        test_data = f.readlines()\n",
    "        test_data = [row.replace('\\n', '') for row in test_data]\n",
    "        reader = csv.reader(test_data, delimiter='\\t')\n",
    "        final_data = []\n",
    "        for row, predicted_value in zip(reader, prediction):\n",
    "            row.append(predicted_value)\n",
    "            final_data.append(row)\n",
    "df = pd.DataFrame.from_records(\n",
    "    final_data,\n",
    "    columns=['title', 'actual_prediction', 'predicted_value'])\n",
    "print(df.head(15).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F. Prediction of unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                title sentiment\n",
      "0   Shooting in Brazil kills at least 14 in dance ...  negative\n",
      "1   Prez calls for action to harness Nepal’s huge ...  negative\n",
      "2                MaHa’s Satru Gate in post-production  negative\n",
      "3   Kathakanksha 2 culminates in a dance extravaganza  positive\n",
      "4   Itahari Gold Cup: Three Star lift first title ...  negative\n",
      "5                      \"NCS, GHCA register easy wins\"  positive\n",
      "6            Forward Gharti Magar to join Mohun Bagan  negative\n",
      "7               Nepse index posts loss of 1.99 points  positive\n",
      "8   India drags feet in building new cross-border ...  positive\n",
      "9               Politicians flout rules to build road  negative\n",
      "10  Expansion Plan: Kaligandaki road corridor set ...  negative\n",
      "11                                  His secret garden  positive\n",
      "12                              Rising from the ashes  negative\n",
      "13                                        Karnali 2.0  negative\n",
      "14                                  The NEA unplugged  negative\n",
      "15                            Minister for Loneliness  negative\n",
      "16                                   A different lens  negative\n",
      "17  Ten die in separate road accidents in past 24 ...  negative\n",
      "18          \"Fire destroys 17 houses in Humla, Rolpa\"  positive\n",
      "19         Parsa National Park welcomes baby elephant  positive\n",
      "20        Rukumkot folk protest over district HQ pick  positive\n",
      "21  Constitution unlikely to be revised soon: UML ...  negative\n",
      "22  \"Govt’s controversial decisions will be revoke...  negative\n",
      "23       Deuba faces ire in party over govt decisions  negative\n",
      "24            UML disabled member’s candidacy revoked  positive\n",
      "25  People in remote Humla face hardships to buy salt  negative\n",
      "26           \"28 hotels, restaurant operators booked\"  negative\n",
      "27  At least 41 people killed in South Korea hospi...  negative\n",
      "28  Lalu Prasad Yadav sentenced to 5 years in jail...  positive\n",
      "29  Indonesian office workers flee as quake shakes...  negative\n",
      "30  Bus fire kills 52 Uzbeks travelling in Kazakhstan  positive\n",
      "31           Baghdad double suicide attack kills many  negative\n",
      "32  Four judges of India's top court criticize its...  positive\n",
      "33    Fugitive robber ‘Tike’ hurt in police crossfire  negative\n",
      "34  Three weather radars being installed for bette...  positive\n",
      "35      \"Dalits, single women to get pay earlier too\"  negative\n",
      "36  Left bloc close to deal on sharing provincial ...  negative\n",
      "37      Inmate on the run injured in police encounter  positive\n",
      "38    Warm clothes distributed to infants in Jajarkot  negative\n",
      "39  'Decisions made by govt will have long-term im...  positive\n",
      "40  Chief Ministers will be appointed by province ...  positive\n",
      "41                                   Rural Connection  negative\n",
      "42  \"6 killed, 5 injured in Dhanusha Tata Sumo jee...  negative\n",
      "43            Six-year-old rescued from Jail in India  negative\n",
      "44                              Police enter quarters  negative\n",
      "45       CAN Info-Tech continues to draw large crowds  negative\n",
      "46     Minister removes secretary over Caan chief row  negative\n",
      "47    Track opens for Mailung-Syabrubesi road section  negative\n",
      "48  Rasuwagadhi border busy despite poor infrastru...  negative\n",
      "49                              Post journalist feted  positive\n",
      "50         Wavering policy stalls solar power project  positive\n",
      "51                                 Poet of the people  positive\n",
      "52                                 Worst in the world  negative\n",
      "53                 Practicing care: An everyday story  negative\n",
      "54                           Nepali art and imitation  positive\n",
      "55                                         In the air  positive\n"
     ]
    }
   ],
   "source": [
    "# let's run our model against news titles that we scraped before\n",
    "with open('linear.bin', 'rb') as model_file, open('news_titles.csv', 'r') as data_file:\n",
    "        data_struct = pickle.load(model_file)\n",
    "        vectorizer, model = data_struct['vectorizer'], data_struct['model']\n",
    "\n",
    "        reader = data_file.readlines()\n",
    "        data = [row.replace('\\n', '') for row in reader]\n",
    "\n",
    "        # vectorize the raw data\n",
    "        new_data_vectors = vectorizer.transform(data)\n",
    "        predictions = model.predict(new_data_vectors)\n",
    "\n",
    "values = pd.Series(['positive' if prediction ==\n",
    "                    '1' else 'negative' for prediction in predictions])\n",
    "output = pd.DataFrame()\n",
    "output['title'] = data\n",
    "output['sentiment'] = values\n",
    "print(output.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### D. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i. Accuracy\n",
    "  - It measures percentage of correct predictions .i.e the number of correct predictions\n",
    " made by our model divided by the total number of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.778\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(news_classifier.test_labels, prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ii. Confusion Matrix\n",
    "  - This matrix helps you to understand the types of errors made by our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[406  94]\n",
      " [128 372]]\n"
     ]
    }
   ],
   "source": [
    "conf_mat = metrics.confusion_matrix(news_classifier.test_labels, prediction)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here, we have 222 (128+94) misclassified data out of 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAE1CAYAAAD6eWvdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZGUUyIXEUk5JXIRkZT7f9GBJ1E+Y7sUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f65f7d9c9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's plot the matrix so that it will be more easier to see the model performance\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.clf()\n",
    "img = plt.imshow(conf_mat, interpolation='nearest')\n",
    "img.set_cmap('winter_r')\n",
    "class_names = ['Negative', 'Positive']\n",
    "plt.title('Positive : Negative Sentiment Confusion Matrix', fontsize=\"15\", color=\"red\")\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names, rotation=45)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "conf_labels = [['TP', 'FN'], ['FP', 'TN']]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j, i, str(conf_labels[i][j]) + \" = \" + str(conf_mat[i][j]))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " - TP (140) : sentiments our model thinks are \"positive\" and are \"positive\" in reality.\n",
    " - FN (94) : sentiments our model thinks are \"negative\" but they are \"positive\" in reality.\n",
    " - FP (128) : sentiments our model thinks are \"positive\" but they are \"negative\" in reality.\n",
    " - TN (372) : sentiments our model thinks are \"negative\" and are \"negative\" in reality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall\n",
    "##### -  There are total 500 (406 + 94) positive labels. And, our model is successfully identified 406 as positive\n",
    " i.e recall = 406/500 = 0.812  (TP/(TP+FN))\n",
    "  -  Recall measures the ability of the classifier to find all the positive data points.\n",
    "  \n",
    "#### Precision\n",
    "##### -  Out of 534 (406 + 128) predicted positive labels, only 406 are real positives.\n",
    " i.e precision = 406 /534 = 0.76 (TP/(TP+FP))\n",
    "  -  Precision measures the ability of the classifier not to mark a negative data point  as positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

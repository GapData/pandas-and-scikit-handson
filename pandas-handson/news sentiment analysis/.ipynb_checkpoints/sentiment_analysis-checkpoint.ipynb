{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis has been using as a tool to cassify response from your user/customer as 'positive' or 'negative' or even 'neutral'.\n",
    "\n",
    "Before use of sentiment analysis algorithms, people were trying to evaluate those user response based on simple practices like : extracting keywords from the content and restricting their findings only on 'what people are talking about?'.\n",
    "This will never helps you out to answer few important questions 'What people are feeling and thinking about your product?'. That means only the explicit statements/reviews or opinions will have measurable output with your old approach. What would you do with large number of implicit comments?\n",
    "\n",
    "Even though, it is extremely arduous to determine the actual tone from given text, we can try to develop a more accurate version of our older one.\n",
    "\n",
    "In the following blog, I am going to help you to understand the basic norms natural language processing in order solve the problem of sentiment analysis. I have divided the whole content into following sub-topics:\n",
    "\t\t1. Problem Statement\n",
    "\t\t2. Problem Analysis\n",
    "\t\t2. Data Collection\n",
    "\t\t3. Model Development\n",
    "\t\t4. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Problem Statement:\n",
    "\n",
    "Though there are lots of challenges out there that can be solved using data science, I come up with a very basic problem. Why I am choosing this -- simply because I do not want to waste my energy just by thinking about big problems and also not to waste your energy to read it and forget in couple of hours :). \n",
    "\n",
    "PROBLEM: We have to classify the sentiment of news titles (Whether positive or negative.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Analysis:\n",
    "\n",
    "For this part, you should be able to answer following questions:\n",
    "\t   1. What will be the input for your program?\n",
    "\t   2. What output you are expecting ?\n",
    "          Whether it is continous value (something like given by regression model?) or   it is a label value (like spam or ham?)\n",
    "\t   3. What sort of problem is this?\n",
    "          Whether it is supervised or unsupervised learning? Is it a problem of classification or  clustering ? \n",
    "\t   4. What are the features you should take into account?\n",
    "\n",
    "\tAs we already know, the sentiment of any text can be classified into\n",
    "    three major categories (positive, negative, and neutral), the problem\n",
    "    is clearly a type of classification. And, we will be using labelled\n",
    "    data (supervised learning).\n",
    "\n",
    "\tYour ML model will not be able to predict the correctly if you don't\n",
    "    have enough training data.\n",
    "\n",
    "\tSuppose, you feed following two texts into your algorithm,\n",
    "\n",
    "\t'New government rule aganist people will' -->Negative\n",
    "\t'Government offers funding opportunities for the startups' -->Positive\n",
    "\n",
    "\tWhat would you expect if the new input text is\n",
    "\n",
    "\t'Tech Companies in Nepal are having problem to raise funds.\n",
    "    Nevertheless, they are doing great with customer acquisition'\n",
    "\n",
    "\tYour model is not being taught with this kind of complex structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collection:\n",
    "\n",
    "After you have your problem analysis, you should only focus couple of your days to gather the related data.\n",
    "\n",
    " a. Training / testing data collection\n",
    "\n",
    "\tI have found labelled data maintained by UCI Machine Learning Repository. The data consists of product reviews and labelled as 1 for positive and 0 for negative responses.\n",
    "\n",
    "\t\thttps://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
    "\n",
    " b. Unseen data collection\n",
    "    Unseen data set is collection of documents wihtout label. We will evaluate our final     model based on unseen data.\n",
    "\tIn my case, I need to collect news titles from the news portals. I chose our national daily paper ( http://www.ekantipur.com/eng) as a data source.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here is the sample code that I have used for scraping news site.\n",
    "# news_spider.py\n",
    "\n",
    "# This module scrolls through news site (ekantipur.com) and collects news titles.\n",
    "import time\n",
    "import csv\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "browser = webdriver.Chrome()\n",
    "\n",
    "browser.get(\"http://www.ekantipur.com/eng\")\n",
    "time.sleep(1)\n",
    "\n",
    "elem = browser.find_element_by_tag_name(\"body\")\n",
    "\n",
    "# set number of pages be scrolled\n",
    "no_of_pagedowns = 5\n",
    "\n",
    "# scroll page (handling infinite page scrolling)\n",
    "while no_of_pagedowns:\n",
    "    elem.send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(0.2)\n",
    "    no_of_pagedowns -= 1\n",
    "\n",
    "content = browser.page_source\n",
    "soup = BeautifulSoup(content, \"html5lib\")\n",
    "browser.close()\n",
    "\n",
    "news_title_containers = soup.find_all(\n",
    "    \"div\", attrs={'class': 'display-news-title'})\n",
    "with open('news_titles.csv', 'w') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for each_container in news_title_containers:\n",
    "        title_link = each_container.find('a')\n",
    "        news_title = title_link.string.strip()\n",
    "        writer.writerow([news_title])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model development\n",
    "\n",
    "Before starting on model development, you should be familiar with few terms that will help you to understand the process:\n",
    "\n",
    "i. Training and Testing\n",
    "\n",
    "This is the process of applying our model on data whose true classes are already known. Once the model is well trained with those data, we need to compute the proportion of the time our classifier will accurately predict the new data. This process is know as testing.\n",
    "\n",
    "ii. Bias(underfitting) and Variance(overfitting)\n",
    "\n",
    "Ideally, whenever we try to develop ML model, we try to capture all the features(information) of the training data and also want our model to generalize the output for unseen input data.\n",
    "\n",
    "<img src=\"files/final_draw.png\">\n",
    "In the diagram above, the linear model is underfit\n",
    " - The model doesn't take care of all the information provided (high bias)\n",
    " - If you provide new data to this model, it will not change it's behavior(shape) (low variance)\n",
    "\n",
    "\n",
    "On the other hand, the model with high degree polynomial at the right is overfit\n",
    " - The model will take into account all the data points (low bias)\n",
    " - The curve will try to change it's shape in order to capture new data points, loosing the generality of the model (high variance)\n",
    "\n",
    "\n",
    "iii. Cross Validation:\n",
    "\n",
    "Most of the time, when we are developing ML model we simply split the data randomly into train and test set and feed into our model. While doing so, we may have a high accuracy on that particular chunk of training data set. What if your model perform differently and give lower accuracy while feeding different chuncks of from the sama data points? So, in cross-validation, we will be creating number of train/test splits and measure the accuracy by calculating average on each of the spits.\n",
    "\n",
    "\n",
    "iv. Learning Curve\n",
    "\t\n",
    "  Learning curve helps to measure the performance of your model based on variations on your training data supply.\n",
    "\n",
    "   Why you need to plot learning curve?\n",
    "   - You can identify the correct spot on your curve where bias and variance is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have divided the whole model development process into following subprocesses:\n",
    "\n",
    " #### A. Loading data / Split into training and test set\n",
    "\n",
    "I have divided the corpus provided by UCI Machine Learning Repository for training and testing as \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "TRAIN_CORPUS_FILES = ['amazon.txt', 'imdb.txt']\n",
    "TEST_CORPUS_FILES = ['yelp.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Feature Extraction\n",
    "  i. We need to convert text data into numerical feature vectors\n",
    "  \n",
    "  Most of the algorithms expect numerical data rather than raw (a sequence of symbols). This process is called vectorization -- turning text documents into numerical representation. This whole process comprises of - tokenization, counting and normalization of data ( More specifically called prcess of Bag of Words). The document will be represented by the occurrences of words in the document rather than the relative position of them.\n",
    "  \n",
    "Let us have a closer look into vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_data = ['Dispute delays National Assembly formation process',\n",
    "              'Country is looking to encourage entrepreneurs and startup process',\n",
    "              'Airline fuel surcharges to go up from Tuesday'] \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# instantiate Vectorizer\n",
    "vec = CountVectorizer()\n",
    "\n",
    "# feed/learn the 'vocabulary' of the training data\n",
    "vec.fit(sample_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the definition above, the `CountVectorizer` simply make whole words lowercase, remove duplicates, and remove single word characters as suggested by regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airline',\n",
       " 'and',\n",
       " 'assembly',\n",
       " 'country',\n",
       " 'delays',\n",
       " 'dispute',\n",
       " 'encourage',\n",
       " 'entrepreneurs',\n",
       " 'formation',\n",
       " 'from',\n",
       " 'fuel',\n",
       " 'go',\n",
       " 'is',\n",
       " 'looking',\n",
       " 'national',\n",
       " 'process',\n",
       " 'startup',\n",
       " 'surcharges',\n",
       " 'to',\n",
       " 'tuesday',\n",
       " 'up']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# current fitted vocabulary\n",
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a document-term matrix. This simply describes the occurence of terms collection of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x21 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 23 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform training data into a 'document-term matrix'\n",
    "sample_train_dtm = vec.transform(sample_train_data)\n",
    "sample_train_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the output, we can see that it creates a matrix with 3 rows and 21 columns.\n",
    " - Here, 3 colums because we have three documents all together\n",
    " - And, 21 rows because we have 21 terms (learned during fitting step above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's observe the dense matrix\n",
    "sample_train_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>and</th>\n",
       "      <th>assembly</th>\n",
       "      <th>country</th>\n",
       "      <th>delays</th>\n",
       "      <th>dispute</th>\n",
       "      <th>encourage</th>\n",
       "      <th>entrepreneurs</th>\n",
       "      <th>formation</th>\n",
       "      <th>from</th>\n",
       "      <th>...</th>\n",
       "      <th>go</th>\n",
       "      <th>is</th>\n",
       "      <th>looking</th>\n",
       "      <th>national</th>\n",
       "      <th>process</th>\n",
       "      <th>startup</th>\n",
       "      <th>surcharges</th>\n",
       "      <th>to</th>\n",
       "      <th>tuesday</th>\n",
       "      <th>up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   airline  and  assembly  country  delays  dispute  encourage  entrepreneurs  \\\n",
       "0        0    0         1        0       1        1          0              0   \n",
       "1        0    1         0        1       0        0          1              1   \n",
       "2        1    0         0        0       0        0          0              0   \n",
       "\n",
       "   formation  from ...  go  is  looking  national  process  startup  \\\n",
       "0          1     0 ...   0   0        0         1        1        0   \n",
       "1          0     0 ...   0   1        1         0        1        1   \n",
       "2          0     1 ...   1   0        0         0        0        0   \n",
       "\n",
       "   surcharges  to  tuesday  up  \n",
       "0           0   0        0   0  \n",
       "1           0   1        0   0  \n",
       "2           1   1        1   1  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us see the complete data map using pandas\n",
    "import pandas as pd\n",
    "pd.DataFrame(sample_train_dtm.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that most of the feature values are `ZEROs`. Why is that? Because most of the documents can be represented by small sets of words. Suppose, you have collected almost 100 reviews (documents) from a site. Then, there could be all together 1000 unique words (corpus) and a single review (single document) might have 10 unique words. right?\n",
    "\n",
    "Storing such a big number of `ZEROs` does not make sense. So, scikit-learn internally uses `scipy.sparse` package to store such a matrix in memory and it also speed up the operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let us now test our model with new example \n",
    "sample_test_data = ['Country is looking to encourage agriculture schemes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data into a document-term matrix (using existing vocabulary)\n",
    "sample_test_dtm = vec.transform(sample_test_data)\n",
    "sample_test_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>and</th>\n",
       "      <th>assembly</th>\n",
       "      <th>country</th>\n",
       "      <th>delays</th>\n",
       "      <th>dispute</th>\n",
       "      <th>encourage</th>\n",
       "      <th>entrepreneurs</th>\n",
       "      <th>formation</th>\n",
       "      <th>from</th>\n",
       "      <th>...</th>\n",
       "      <th>go</th>\n",
       "      <th>is</th>\n",
       "      <th>looking</th>\n",
       "      <th>national</th>\n",
       "      <th>process</th>\n",
       "      <th>startup</th>\n",
       "      <th>surcharges</th>\n",
       "      <th>to</th>\n",
       "      <th>tuesday</th>\n",
       "      <th>up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   airline  and  assembly  country  delays  dispute  encourage  entrepreneurs  \\\n",
       "0        0    0         0        1       0        0          1              0   \n",
       "\n",
       "   formation  from ...  go  is  looking  national  process  startup  \\\n",
       "0          0     0 ...   0   1        1         0        0        0   \n",
       "\n",
       "   surcharges  to  tuesday  up  \n",
       "0           0   1        0   0  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the vocabulary and document-term matrix together\n",
    "pd.DataFrame(sample_test_dtm.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that `agriculture` and  `schemes` are not the part of output. For our model, to make prediction about this new observation, this new data must have the same features as the training observations. We did not train on the features \"`agriculture` and  `schemes`\" so our model would not be able to predict. Make any sense ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Problem with `CountVectorizer` **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If are looking at two different documents talking about same topic, one bit longer than\n",
    "other, you will find the average count values in the longer one will be higher than\n",
    "the shorter document. That means, `CountVectorizer` simply gives equal weightage to each\n",
    "of the word present in your document (one word represents one column in DataFrame). And,\n",
    "when you have new document, it simply maps 1 if the word is present in that document\n",
    "otherwise map to 0.\n",
    "\n",
    "So as to avoid this problem, we can use `TfidfTransformer` which evaluates frequencies of \n",
    "words rather than their occurences in the document and helps model to figure out which of\n",
    "the word is more important in the document and also with respect to the corpus.\n",
    "\n",
    "\n",
    "#### Let's  take a closer look at `Tfidf`\n",
    "\n",
    "Suppose you have a corpus of documents talking about `Mountains`. Now, `Tfidf` will work in three different parts\n",
    "\n",
    "#### i. Term Frequency (Tf)\n",
    "  It simply looks for the number of times a particular term occurs in your single document.\n",
    "  Let's take an example. You can find word `is` in almost all of your documents. Do you think the word `is` has any credit to describe your document ? But, at the same time, if you find word `Everest` in the document, you can figure out that the document is about `Mountains`.\n",
    " So, we need to find a way to reduce the weightage of word `is`. One of the method is calculating reciprocal of the count value, which minimizes the significant of word `is` in the document. But, you might face another problem here. Suppose, you encounter a word in your document (let's say `stratovolcano` having rare occurence in your document). If you take reciprocal of this occurence, you will get 1 (or close to 1 - hightest weightage). But, this word does not give much sense about `Montains`.\n",
    " \n",
    "For now, we can keep the word count as follows:\n",
    "\n",
    "    `tf(\"is\") = 1000`\n",
    "    `tf(\"Everest\") = 50`\n",
    "    `tf(\"stratovolcano\") = 2`\n",
    "\n",
    "\n",
    "#### ii. Inverser Document Frequency (iDF)\n",
    "\n",
    "  The problem of occurence of rare and more frequent words will be handled by this method.\n",
    "   Inverser Document Frequency gives downscale weights for words that occur in many documents in the corpus by taking log of number of total documents in your corpus divided by the total number of documents having occurence of the word.\n",
    "   i.e iDF = log (total number of documents/ total documents with word occurence)\n",
    "   \n",
    "   Let's calculate iDF for above words. (Suppose we have total 20 documents)\n",
    "   \n",
    "    `iDF(\"is\") = log(20/20) = 0` , Since 'is' occurs in all the documents\n",
    "    `iDF(\"Everest\") = log(20/5) = 0.6`,  Since corpus is talking about 'Mountains'\n",
    "    `iDF(\"stratovolcano\") = log(10/1) = 1 `, Since `stratovolcano` occurs in one doc.\n",
    "   \n",
    "#### iii. TfiDF\n",
    "    \n",
    "    TfiDF = TF * iDF\n",
    "    \n",
    "    Therefore,\n",
    "        `TfiDF(\"is\") = 1000 * 0 = 0`\n",
    "        `TfiDF(\"Everest\") = 50 * 0.6 = 30`\n",
    "        `TfiDF(\"stratovolcano\") = 2 * 1 = 2`\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calcuate TfiDF for our sample train data above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "sample_train_tfidf = tfidf_vec.fit_transform(sample_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>and</th>\n",
       "      <th>assembly</th>\n",
       "      <th>country</th>\n",
       "      <th>delays</th>\n",
       "      <th>dispute</th>\n",
       "      <th>encourage</th>\n",
       "      <th>entrepreneurs</th>\n",
       "      <th>formation</th>\n",
       "      <th>from</th>\n",
       "      <th>...</th>\n",
       "      <th>go</th>\n",
       "      <th>is</th>\n",
       "      <th>looking</th>\n",
       "      <th>national</th>\n",
       "      <th>process</th>\n",
       "      <th>startup</th>\n",
       "      <th>surcharges</th>\n",
       "      <th>to</th>\n",
       "      <th>tuesday</th>\n",
       "      <th>up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.322002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350139</td>\n",
       "      <td>0.350139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350139</td>\n",
       "      <td>0.350139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266290</td>\n",
       "      <td>0.350139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.363255</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363255</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363255</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363255</td>\n",
       "      <td>0.276265</td>\n",
       "      <td>0.363255</td>\n",
       "      <td>0.363255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    airline       and  assembly   country    delays   dispute  encourage  \\\n",
       "0  0.000000  0.000000  0.423394  0.000000  0.423394  0.423394   0.000000   \n",
       "1  0.000000  0.350139  0.000000  0.350139  0.000000  0.000000   0.350139   \n",
       "2  0.363255  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "\n",
       "   entrepreneurs  formation      from    ...           go        is   looking  \\\n",
       "0       0.000000   0.423394  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "1       0.350139   0.000000  0.000000    ...     0.000000  0.350139  0.350139   \n",
       "2       0.000000   0.000000  0.363255    ...     0.363255  0.000000  0.000000   \n",
       "\n",
       "   national   process   startup  surcharges        to   tuesday        up  \n",
       "0  0.423394  0.322002  0.000000    0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.000000  0.266290  0.350139    0.000000  0.266290  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.000000    0.363255  0.276265  0.363255  0.363255  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(sample_train_tfidf.toarray(), columns=tfidf_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Choose estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using `SVM` as my estimator. We will not talk details on SVM because it might take long hours to understand the algorithms.\n",
    "In brief, `linear SVM` tries to find an optimal vector ( a line) sperating two data points i.e classifying data. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

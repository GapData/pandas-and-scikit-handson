{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis has been using as a tool to cassify response from your user/customer as 'positive' or 'negative' or even 'neutral'. It combines two different disciplines : Natural Language Processing and Text Analysis to extract information from text data. In sentiment analysis, algorithm will learn from labelled example data and predict the label of new /unseen data points. This approach is called supervised learning, as we will train our model with corpus of labelled news.\n",
    "\n",
    "### Why sentiment analysis?\n",
    "\n",
    "People have different ways to express their attitudes or opinions / reviews towards your product /event / movie or even for people. These user reviews have potential to build brand authenticity between customers and even to establish trust in product.Most of the companies are trying to evaluate the brand value of a product based on customer reviews. In this process, company can extract customer behaviors like : which products are popular among users, what percentage of your customer dislike your brand, how many users have postive sentiment on their competitors product? These questions ultimately help companies to focus on different aspects of product like : product revamp , new marketing strategy and even for social media outreach plan.\n",
    "\n",
    "Before use of sentiment analysis algorithms, people were trying to evaluate those user response based on simple practices like : extracting keywords from the content and restricting their findings only on 'what people are talking about?'.\n",
    "This will never helps you out to answer few important questions 'What people are feeling and thinking about your product?'. That means only the explicit statements/reviews or opinions will have measurable output with your old approach. What would you do with large number of implicit comments?\n",
    "\n",
    "Even though, it is extremely arduous to determine the actual tone from given text, we can try to develop a more accurate version of our older one.\n",
    "\n",
    "### What tools we are using?\n",
    "\n",
    " - Language : python\n",
    " - Scraping library : BeautifulSoup\n",
    " - Machine learning library : scikit-learn \n",
    " - Data wrangling : Pandas and Numpy\n",
    " - Plotting : matplotlib\n",
    "\n",
    "\n",
    "In the following blog, I am going to help you to understand the basic norms of natural language processing in order solve the problem of sentiment analysis. I have divided the whole content into following sub-topics:\n",
    "\t\t1. Problem Statement\n",
    "\t\t2. Problem Analysis\n",
    "\t\t3. Data Collection\n",
    "\t\t4. Model Development\n",
    "\t\t5. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Problem Statement:\n",
    "\n",
    "Though there are lots of challenges out there that can be solved using data science, I come up with a very basic problem. Why I am choosing this -- simply because I do not want to waste my energy just by thinking about big problems and also not to waste your energy to read it and forget in couple of hours :). \n",
    "\n",
    "PROBLEM: We will classify the sentiment of news titles (Whether positive or negative or neutral.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Analysis:\n",
    "\n",
    "For this part, we will answer following questions:\n",
    "\t   1. What will be the input for your program?\n",
    "\t   2. What output you are expecting ?\n",
    "          Whether it is continous value (something like given by regression model?) or   it is a label value (like spam or ham?)\n",
    "\t   3. What sort of problem is this?\n",
    "          Whether it is supervised or unsupervised learning? Is it a problem of classification or  clustering ? \n",
    "\t   4. What are the features you should take into account?\n",
    "\n",
    "\tAs we already know, the sentiment of any text can be classified into\n",
    "    three major categories (positive, negative, and neutral), the problem\n",
    "    is clearly a type of classification. And, we will be using labelled\n",
    "    data (supervised learning).\n",
    "\n",
    "\tYour ML model will not be able to predict the correctly if you don't\n",
    "    have enough training data.\n",
    "\n",
    "\tSuppose, you feed following two texts into your algorithm,\n",
    "\n",
    "\t'New government rule aganist people will' -->Negative\n",
    "\t'Government offers funding opportunities for the startups' -->Positive\n",
    "\n",
    "\tWhat would you expect if the new input text is\n",
    "\n",
    "\t'Tech Companies in Nepal are having problem to raise funds.\n",
    "    Nevertheless, they are doing great with customer acquisition'\n",
    "\n",
    "\tYour model is not being taught with this kind of complex structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collection:\n",
    "\n",
    "After you have your problem analysis, you should only focus couple of your days to gather the related data.\n",
    "\n",
    " a. Training / testing data collection\n",
    "\n",
    "\tI have collected data from news portal ekantipur.com for training/testing of our sentiment model.\n",
    "\n",
    " b. Unseen data collection\n",
    "    Unseen data set is collection of documents wihtout label. We will evaluate our final     model based on unseen data.\n",
    "\tIn my case, I need to collect news titles from the news portals. I chose our national daily paper (setopati.net) as a data source.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here is the sample code that I have used for scraping news site.\n",
    "# news_spider.py\n",
    "\n",
    "# This module scrolls through news site (ekantipur.com) and collects news titles.\n",
    "import time\n",
    "import csv\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "browser = webdriver.Chrome()\n",
    "\n",
    "browser.get(\"http://www.ekantipur.com/eng\")\n",
    "time.sleep(1)\n",
    "\n",
    "elem = browser.find_element_by_tag_name(\"body\")\n",
    "\n",
    "# set number of pages be scrolled\n",
    "no_of_pagedowns = 5\n",
    "\n",
    "# scroll page (handling infinite page scrolling)\n",
    "while no_of_pagedowns:\n",
    "    elem.send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(0.2)\n",
    "    no_of_pagedowns -= 1\n",
    "\n",
    "content = browser.page_source\n",
    "soup = BeautifulSoup(content, \"html5lib\")\n",
    "browser.close()\n",
    "\n",
    "news_title_containers = soup.find_all(\n",
    "    \"div\", attrs={'class': 'display-news-title'})\n",
    "with open('news_titles.csv', 'w') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for each_container in news_title_containers:\n",
    "        title_link = each_container.find('a')\n",
    "        news_title = title_link.string.strip()\n",
    "        writer.writerow([news_title])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model development\n",
    "\n",
    "Before starting on model development, you should be familiar with few terms that will help you to understand the process:\n",
    "\n",
    "i. Training and Testing\n",
    "\n",
    "This is the process of applying our model on data whose true classes are already known. Once the model is well trained with those data, we need to compute the proportion of the time our classifier will accurately predict the new data. This process is know as testing.\n",
    "\n",
    "ii. Bias(underfitting) and Variance(overfitting)\n",
    "\n",
    "Ideally, whenever we try to develop ML model, we try to capture all the features(information) of the training data and also want our model to generalize the output for unseen input data.\n",
    "\n",
    "<img src=\"final_draw.png\">\n",
    "In the diagram above, the linear model is underfit\n",
    " - The model doesn't take care of all the information provided (high bias)\n",
    " - If you provide new data to this model, it will not change it's behavior(shape) (low variance)\n",
    "\n",
    "\n",
    "On the other hand, the model with high degree polynomial at the right is overfit\n",
    " - The model will take into account all the data points (low bias)\n",
    " - The curve will try to change it's shape in order to capture new data points, loosing the generality of the model (high variance)\n",
    "\n",
    "\n",
    "iii. Cross Validation:\n",
    "\n",
    "Most of the time, when we are developing ML model we simply split the data randomly into train and test set and feed into our model. While doing so, we may have a high accuracy on that particular chunk of training data set. What if your model perform differently and give lower accuracy while feeding different chuncks of from the sama data points? So, in cross-validation, we will be creating number of train/test splits and measure the accuracy by calculating average on each of the spits.\n",
    "\n",
    "\n",
    "iv. Learning Curve\n",
    "\t\n",
    "  Learning curve helps to measure the performance of your model based on variations on your training data supply.\n",
    "\n",
    "   Why you need to plot learning curve?\n",
    "   - You can identify the correct spot on your curve where bias and variance is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have divided the whole model development process into following subprocesses:\n",
    "\n",
    " #### A. Loading data / Split into training and test set\n",
    "\n",
    "I have divided the collected data by 80/20 for training and testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Petrol bomb thrown at Suu Kyi’s villa, no injuries</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2 Nepalis killed in India road accident</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Province 3 Assembly meet concludes, sets date for Speaker election</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ton-up Malla hands Nepal second win</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shaktikhor industrial park begins DPR prep</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 text label\n",
       "0  Petrol bomb thrown at Suu Kyi’s villa, no injuries                  -1  \n",
       "1  2 Nepalis killed in India road accident                             -1  \n",
       "2  Province 3 Assembly meet concludes, sets date for Speaker election  0   \n",
       "3  Ton-up Malla hands Nepal second win                                 1   \n",
       "4  Shaktikhor industrial park begins DPR prep                          0   "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\n",
    "    'news_corpus/kantipur.csv',names=['text', 'label'],\n",
    "    dtype={'label': object})\n",
    "data.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['-1', '0', '1'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us remove rows having non label values\n",
    "\n",
    "data = data[data['label'].map(len) <=2]\n",
    "data['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrGp7C9q6DjIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0373d970b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data['label'].value_counts().plot(kind=\"bar\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train/test data split\n",
    "train, test = train_test_split(data, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Feature Extraction\n",
    "  i. We need to convert text data into numerical feature vectors\n",
    "  \n",
    "  Most of the algorithms expect numerical data rather than raw (a sequence of symbols). This process is called vectorization -- turning text documents into numerical representation. This whole process comprises of - tokenization, counting and normalization of data ( More specifically called prcess of Bag of Words). The document will be represented by the occurrences of words in the document rather than the relative position of them.\n",
    "  \n",
    "Let us have a closer look into vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_train_data = ['Dispute delays National Assembly formation process',\n",
    "              'Country is looking to encourage entrepreneurs and startup process',\n",
    "              'Airline fuel surcharges to go up from Tuesday'] \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# instantiate Vectorizer\n",
    "vec = CountVectorizer()\n",
    "\n",
    "# feed/learn the 'vocabulary' of the training data\n",
    "vec.fit(sample_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the definition above, the `CountVectorizer` simply make whole words lowercase, remove duplicates, and remove single word characters as suggested by regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airline',\n",
       " 'and',\n",
       " 'assembly',\n",
       " 'country',\n",
       " 'delays',\n",
       " 'dispute',\n",
       " 'encourage',\n",
       " 'entrepreneurs',\n",
       " 'formation',\n",
       " 'from',\n",
       " 'fuel',\n",
       " 'go',\n",
       " 'is',\n",
       " 'looking',\n",
       " 'national',\n",
       " 'process',\n",
       " 'startup',\n",
       " 'surcharges',\n",
       " 'to',\n",
       " 'tuesday',\n",
       " 'up']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# current fitted vocabulary\n",
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a document-term matrix. This simply describes the occurence of terms collection of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x21 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 23 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform training data into a 'document-term matrix'\n",
    "sample_train_dtm = vec.transform(sample_train_data)\n",
    "sample_train_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the output, we can see that it creates a matrix with 3 rows and 21 columns.\n",
    " - Here, 3 colums because we have three documents all together\n",
    " - And, 21 rows because we have 21 terms (learned during fitting step above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's observe the dense matrix\n",
    "sample_train_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>and</th>\n",
       "      <th>assembly</th>\n",
       "      <th>country</th>\n",
       "      <th>delays</th>\n",
       "      <th>dispute</th>\n",
       "      <th>encourage</th>\n",
       "      <th>entrepreneurs</th>\n",
       "      <th>formation</th>\n",
       "      <th>from</th>\n",
       "      <th>...</th>\n",
       "      <th>go</th>\n",
       "      <th>is</th>\n",
       "      <th>looking</th>\n",
       "      <th>national</th>\n",
       "      <th>process</th>\n",
       "      <th>startup</th>\n",
       "      <th>surcharges</th>\n",
       "      <th>to</th>\n",
       "      <th>tuesday</th>\n",
       "      <th>up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   airline  and  assembly  country  delays  dispute  encourage  entrepreneurs  \\\n",
       "0        0    0         1        0       1        1          0              0   \n",
       "1        0    1         0        1       0        0          1              1   \n",
       "2        1    0         0        0       0        0          0              0   \n",
       "\n",
       "   formation  from ...  go  is  looking  national  process  startup  \\\n",
       "0          1     0 ...   0   0        0         1        1        0   \n",
       "1          0     0 ...   0   1        1         0        1        1   \n",
       "2          0     1 ...   1   0        0         0        0        0   \n",
       "\n",
       "   surcharges  to  tuesday  up  \n",
       "0           0   0        0   0  \n",
       "1           0   1        0   0  \n",
       "2           1   1        1   1  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us see the complete data map using pandas\n",
    "import pandas as pd\n",
    "pd.DataFrame(sample_train_dtm.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that most of the feature values are `ZEROs`. Why is that? Because most of the documents can be represented by small sets of words. Suppose, you have collected almost 100 reviews (documents) from a site. Then, there could be all together 1000 unique words (corpus) and a single review (single document) might have 10 unique words. right?\n",
    "\n",
    "Storing such a big number of `ZEROs` does not make sense. So, scikit-learn internally uses `scipy.sparse` package to store such a matrix in memory and it also speed up the operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let us now test our model with new example \n",
    "sample_test_data = ['Country is looking to encourage agriculture schemes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data into a document-term matrix (using existing vocabulary)\n",
    "sample_test_dtm = vec.transform(sample_test_data)\n",
    "sample_test_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>and</th>\n",
       "      <th>assembly</th>\n",
       "      <th>country</th>\n",
       "      <th>delays</th>\n",
       "      <th>dispute</th>\n",
       "      <th>encourage</th>\n",
       "      <th>entrepreneurs</th>\n",
       "      <th>formation</th>\n",
       "      <th>from</th>\n",
       "      <th>...</th>\n",
       "      <th>go</th>\n",
       "      <th>is</th>\n",
       "      <th>looking</th>\n",
       "      <th>national</th>\n",
       "      <th>process</th>\n",
       "      <th>startup</th>\n",
       "      <th>surcharges</th>\n",
       "      <th>to</th>\n",
       "      <th>tuesday</th>\n",
       "      <th>up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   airline  and  assembly  country  delays  dispute  encourage  entrepreneurs  \\\n",
       "0        0    0         0        1       0        0          1              0   \n",
       "\n",
       "   formation  from ...  go  is  looking  national  process  startup  \\\n",
       "0          0     0 ...   0   1        1         0        0        0   \n",
       "\n",
       "   surcharges  to  tuesday  up  \n",
       "0           0   1        0   0  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the vocabulary and document-term matrix together\n",
    "pd.DataFrame(sample_test_dtm.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that `agriculture` and  `schemes` are not the part of output. For our model, to make prediction about this new observation, this new data must have the same features as the training observations. We did not train on the features \"`agriculture` and  `schemes`\" so our model would not be able to predict. Make any sense ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Problem with `CountVectorizer` **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If are looking at two different documents talking about same topic, one bit longer than\n",
    "other, you will find the average count values in the longer one will be higher than\n",
    "the shorter document. That means, `CountVectorizer` simply gives equal weightage to each\n",
    "of the word present in your document (one word represents one column in DataFrame). And,\n",
    "when you have new document, it simply maps 1 if the word is present in that document\n",
    "otherwise map to 0.\n",
    "\n",
    "So as to avoid this problem, we can use `TfidfTransformer` which evaluates frequencies of \n",
    "words rather than their occurences in the document and helps model to figure out which of\n",
    "the word is more important in the document and also with respect to the corpus.\n",
    "\n",
    "\n",
    "#### Let's  take a closer look at `Tfidf`\n",
    "\n",
    "Suppose you have a corpus of documents talking about `Mountains`. Now, `Tfidf` will work in three different parts\n",
    "\n",
    "#### i. Term Frequency (Tf)\n",
    "  It simply looks for the number of times a particular term occurs in your single document.\n",
    "  Let's take an example. You can find word `is` in almost all of your documents. Do you think the word `is` has any credit to describe your document ? But, at the same time, if you find word `Everest` in the document, you can figure out that the document is about `Mountains`.\n",
    " So, we need to find a way to reduce the weightage of word `is`. One of the method is calculating reciprocal of the count value, which minimizes the significant of word `is` in the document. But, you might face another problem here. Suppose, you encounter a word in your document (let's say `stratovolcano` having rare occurence in your document). If you take reciprocal of this occurence, you will get 1 (or close to 1 - hightest weightage). But, this word does not give much sense about `Montains`.\n",
    " \n",
    "For now, we can keep the word count as follows:\n",
    "\n",
    "    `tf(\"is\") = 1000`\n",
    "    `tf(\"Everest\") = 50`\n",
    "    `tf(\"stratovolcano\") = 2`\n",
    "\n",
    "\n",
    "#### ii. Inverser Document Frequency (iDF)\n",
    "\n",
    "  The problem of occurence of rare and more frequent words will be handled by this method.\n",
    "   Inverser Document Frequency gives downscale weights for words that occur in many documents in the corpus by taking log of number of total documents in your corpus divided by the total number of documents having occurence of the word.\n",
    "   i.e iDF = log (total number of documents/ total documents with word occurence)\n",
    "   \n",
    "   Let's calculate iDF for above words. (Suppose we have total 20 documents)\n",
    "   \n",
    "    `iDF(\"is\") = log(20/20) = 0` , Since 'is' occurs in all the documents\n",
    "    `iDF(\"Everest\") = log(20/5) = 0.6`,  Since corpus is talking about 'Mountains'\n",
    "    `iDF(\"stratovolcano\") = log(10/1) = 1 `, Since `stratovolcano` occurs in one doc.\n",
    "   \n",
    "#### iii. TfiDF\n",
    "    \n",
    "    TfiDF = TF * iDF\n",
    "    \n",
    "    Therefore,\n",
    "        `TfiDF(\"is\") = 1000 * 0 = 0`\n",
    "        `TfiDF(\"Everest\") = 50 * 0.6 = 30`\n",
    "        `TfiDF(\"stratovolcano\") = 2 * 1 = 2`\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calcuate TfiDF for our sample train data above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "sample_train_tfidf = tfidf_vec.fit_transform(sample_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>and</th>\n",
       "      <th>assembly</th>\n",
       "      <th>country</th>\n",
       "      <th>delays</th>\n",
       "      <th>dispute</th>\n",
       "      <th>encourage</th>\n",
       "      <th>entrepreneurs</th>\n",
       "      <th>formation</th>\n",
       "      <th>from</th>\n",
       "      <th>...</th>\n",
       "      <th>go</th>\n",
       "      <th>is</th>\n",
       "      <th>looking</th>\n",
       "      <th>national</th>\n",
       "      <th>process</th>\n",
       "      <th>startup</th>\n",
       "      <th>surcharges</th>\n",
       "      <th>to</th>\n",
       "      <th>tuesday</th>\n",
       "      <th>up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.322002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350139</td>\n",
       "      <td>0.350139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350139</td>\n",
       "      <td>0.350139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266290</td>\n",
       "      <td>0.350139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.363255</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363255</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363255</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363255</td>\n",
       "      <td>0.276265</td>\n",
       "      <td>0.363255</td>\n",
       "      <td>0.363255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    airline       and  assembly   country    delays   dispute  encourage  \\\n",
       "0  0.000000  0.000000  0.423394  0.000000  0.423394  0.423394   0.000000   \n",
       "1  0.000000  0.350139  0.000000  0.350139  0.000000  0.000000   0.350139   \n",
       "2  0.363255  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "\n",
       "   entrepreneurs  formation      from    ...           go        is   looking  \\\n",
       "0       0.000000   0.423394  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "1       0.350139   0.000000  0.000000    ...     0.000000  0.350139  0.350139   \n",
       "2       0.000000   0.000000  0.363255    ...     0.363255  0.000000  0.000000   \n",
       "\n",
       "   national   process   startup  surcharges        to   tuesday        up  \n",
       "0  0.423394  0.322002  0.000000    0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.000000  0.266290  0.350139    0.000000  0.266290  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.000000    0.363255  0.276265  0.363255  0.363255  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(sample_train_tfidf.toarray(), columns=tfidf_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us discuss few questions regarding feature engineering.\n",
    "\n",
    "#### Why do you need to vectorize the words?\n",
    " - Raw text data can not be directly fed into machine learning model. So, text to vector conversion PIPE-line consists of following process:\n",
    "\n",
    "    * Tokenization : When we convert our text into list of words by removing stop words and punctuations, we assign individual string an id value.\n",
    "     \n",
    "    * Frequency counting :  We count the occurence of individual token into each of the document.\n",
    "    * Normalizing :  We reduce the importance of token having frequent occurence in majority of documents by giving it lower weight.\n",
    "\n",
    "  At the end, the token occurrene frequency is our feature for the model.reated as a feature.\n",
    "  \n",
    "####  Why do you need to do data standardization?\n",
    "   - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Choose estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using `SVM` as my estimator. We will not talk details on SVM because it might take long hours to understand the algorithms.\n",
    "In brief, `linear SVM` tries to find an optimal vector ( a line) sperating two data points i.e classifying data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# news_classifier.py\n",
    "import pickle\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "\n",
    "class NewsClassification:\n",
    "    def __init__(self):\n",
    "        self.train_data = []\n",
    "        self.train_labels = []\n",
    "        self.test_data = []\n",
    "        self.test_labels = []\n",
    "\n",
    "        self.train_vectors = None\n",
    "        self.test_vectors = None\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.model = None\n",
    "\n",
    "    def prepare_feature_vectors(self):\n",
    "        # Train the feature vectors\n",
    "        self.train_vectors = self.vectorizer.fit_transform(self.train_data)\n",
    "        # Apply model on test data\n",
    "        # : since they have already been fit to the training set\n",
    "        self.test_vectors = self.vectorizer.transform(self.test_data)\n",
    "\n",
    "    def prepare_model(self):\n",
    "        self.model = svm.SVC(kernel='linear', class_weight=\"balanced\")\n",
    "        self.model.fit(self.train_vectors, self.train_labels)\n",
    "\n",
    "        # let's save our model using pickle.\n",
    "        # we will using this model later for unseen data as well.\n",
    "        data_struct = {'vectorizer': self.vectorizer, 'model': self.model}\n",
    "        with open('%s.bin' % 'linear', 'wb') as f:\n",
    "            pickle.dump(data_struct, f)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    news_classifier = NewsClassification()\n",
    "    data = pd.read_csv(\n",
    "        'news_corpus/kantipur.csv',\n",
    "        names=['text', 'label'], dtype={'label': object})\n",
    "\n",
    "    data = data[data['label'].map(len) <= 2]\n",
    "    train, test = train_test_split(data, test_size=0.2)\n",
    "\n",
    "    news_classifier = NewsClassification()\n",
    "    news_classifier.train_data, news_classifier.train_labels =\\\n",
    "        train['text'], train['label']\n",
    "    news_classifier.test_data, news_classifier.test_labels =\\\n",
    "        test['text'], test['label']\n",
    "\n",
    "    news_classifier.prepare_feature_vectors()\n",
    "    news_classifier.prepare_model()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. Prediction with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leapfrong/workspace/envs/envpractice/lib/python3.5/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3173</th>\n",
       "      <td>Spritz text streaming technology increases reading speed</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680</th>\n",
       "      <td>I must say I have taped most of the episodes and i find myself watching them over and over again.</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>LASANAA’s latest art residency concludes with an exhibit</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>Indian envoy Puri calls on PM Deuba</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2681</th>\n",
       "      <td>Stock futures down slightly; Coca-Cola cuts executive pay; Chiquita top banana  ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                     text  \\\n",
       "3173  Spritz text streaming technology increases reading speed                                              \n",
       "1680  I must say I have taped most of the episodes and i find myself watching them over and over again.     \n",
       "1373  LASANAA’s latest art residency concludes with an exhibit                                              \n",
       "814   Indian envoy Puri calls on PM Deuba                                                                   \n",
       "2681  Stock futures down slightly; Coca-Cola cuts executive pay; Chiquita top banana  ...                   \n",
       "\n",
       "     label prediction  \n",
       "3173  1     1          \n",
       "1680  1     -1         \n",
       "1373  1     1          \n",
       "814   0     0          \n",
       "2681  -1    1          "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's run prediction with our test data\n",
    "prediction = news_classifier.model.predict(news_classifier.test_vectors)\n",
    "\n",
    "test['prediction'] = prediction\n",
    "\n",
    "test.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E. Prediction of unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VP Pun stresses on making 21st century as SAARC era</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oli, Swaraj have 45-min one-on-one meeting at Soaltee</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kenya’s High Court orders government’s TV shutdown to end</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UML directs its National Assembly voters not to leave country, provinces until election</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Swaraj arrives in Kathmandu</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                      text  \\\n",
       "0  VP Pun stresses on making 21st century as SAARC era                                       \n",
       "1  Oli, Swaraj have 45-min one-on-one meeting at Soaltee                                     \n",
       "2  Kenya’s High Court orders government’s TV shutdown to end                                 \n",
       "3  UML directs its National Assembly voters not to leave country, provinces until election   \n",
       "4  Swaraj arrives in Kathmandu                                                               \n",
       "\n",
       "  sentiment  \n",
       "0  positive  \n",
       "1  negative  \n",
       "2  neutral   \n",
       "3  negative  \n",
       "4  neutral   "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's run our model against news titles that we scraped before\n",
    "with open('linear.bin', 'rb') as model_file, open('news_corpus/setopati.csv', 'r') as data_file:\n",
    "        data_struct = pickle.load(model_file)\n",
    "        vectorizer, model = data_struct['vectorizer'], data_struct['model']\n",
    "\n",
    "        reader = data_file.readlines()\n",
    "        data = [row.replace('\\n', '') for row in reader]\n",
    "\n",
    "        # vectorize the raw data\n",
    "        new_data_vectors = vectorizer.transform(data)\n",
    "        predictions = model.predict(new_data_vectors)\n",
    "\n",
    "values = pd.Series(['positive' if prediction ==\n",
    "                    '1' else 'negative' if prediction ==  '-1'\n",
    "                    else 'neutral' for prediction in predictions])\n",
    "output = pd.DataFrame()\n",
    "output['text'] = data\n",
    "output['sentiment'] = values\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "output.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Model Evaluation\n",
    "\n",
    "There are different appoaches evaluating for model performance and error analysis. And, this particularly depends upon type of problem we are solving. Since our problem falls under classification, we need to identify which class defines positive result and which one negative. In our case, we assume that predicting 'positive' class is a positive result. \n",
    "If our model is predicting input text as 'positive' and it really has 'positive' label in test data, then it is called True positive. Why True Positive? 'True' because the real class of the input data matched the prediction, and 'Positive' as we have assumed predicting 'positive' is Postive result above.\n",
    "On the other hand, if our model predicted an input text as 'positive' though it is 'negative' in real, it is called False Positive. 'False' because the real class of the input data did not match with the predicted value, and 'Positive' as prediction 'positve' class matched our assumption above.\n",
    "\n",
    "While deriving model performance and error analysis, we should always consider test data. Using training data will not make much sense as our model will be baised to give the actual prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i. Accuracy\n",
    "  - It measures percentage of correct predictions .i.e the number of correct predictions\n",
    " made by our model divided by the total number of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.669738863287\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(news_classifier.test_labels, prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Cross validation accuracy\n",
    "   -  `cross_val_score` splits the data into say 10 folds. And, it fits with 9 folds into your model and find scores with 10th fold data. Then it gives you the 10 scores from which you can calculate a mean for average score of total folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.681921762611\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "results_svm_cv = cross_val_score(\n",
    "        news_classifier.model,\n",
    "        news_classifier.train_vectors,\n",
    "        news_classifier.train_labels,\n",
    "        cv=10, scoring='accuracy')\n",
    "print(results_svm_cv.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  NOTE : Test data accuracy is near to cross validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### iii. Confusion Matrix\n",
    "  - This matrix helps you to understand the types of errors made by our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[168  56  39]\n",
      " [ 35 139  31]\n",
      " [ 22  32 129]]\n"
     ]
    }
   ],
   "source": [
    "conf_mat = metrics.confusion_matrix(\n",
    "    news_classifier.test_labels, prediction,\n",
    "    labels=['1', '-1', '0']\n",
    ")\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here, we have 222 (128+94) misclassified data out of 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAE1CAYAAAD6eWvdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f036f8f3e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's plot the matrix so that it will be more easier to see the model performance\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.clf()\n",
    "img = plt.imshow(conf_mat, interpolation='nearest')\n",
    "img.set_cmap('winter_r')\n",
    "class_names = ['Positive', 'Negative', 'Neutral']\n",
    "plt.title('Positive : Negative Sentiment Confusion Matrix', fontsize=\"15\", color=\"red\")\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names, rotation=45)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "conf_labels = [['TP', 'FN'], ['FP', 'TN']]\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        plt.text(j, i, str(conf_mat[i][j]))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "TP = True positive , FN = False Negative, \n",
    " - TP_A (168) : sentiments our model thinks are \"positive\" and are \"positive\" in reality.\n",
    " - FN_A (56 + 39 = 95) : sentiments our model thinks are \"negative\" or \"neutral\" but they are \"positive\".\n",
    " - FP_A (35 + 22 = 57) : sentiments our model thinks are \"positive\" but they are \"negative\" or \"neutral\" in reality.\n",
    " - TP_B (139) : sentiments our model thinks are \"negative\" and are \"negative\" in reality.\n",
    " - FN_B (35 + 31 = 66) : sentiments our model thinks are \"positive\" or \"neutral\" but they are \"negative\" in reality.\n",
    " - FP_B (56 + 32 = 88) : sentiments our model thinks are \"negative\" but they are \"positive\" or \"neutral\" in reality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall\n",
    "##### -  There are total 217 (122 + 95) positive labels. And, our model is successfully identified 168 as positive\n",
    " i.e recall = 168/217 = 0.774  (TP/(TP+FN))\n",
    "  -  Recall measures the ability of the classifier to find all the positive data points.\n",
    "  \n",
    "#### Precision\n",
    "##### -  Out of 225 (168 + 57) predicted positive labels, only 168 are real positives.\n",
    " i.e precision = 168 /225 = 0.746 (TP/(TP+FP))\n",
    "  -  It simply measures the proportion of correct positive predictions out of all positive predictions made by our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "iv. Hypterparamter tuning\n",
    "  - Until now, we are looking at training data to measure the accuracy of our model.\n",
    "    But, there are also other parameters which are not the part of model\n",
    "    training process. These parameters are called `hyperparameters`,\n",
    "    which defines areas like - the complexity of model and how fast\n",
    "    the model should learn.\n",
    "    So as to select best parameters, we simply set different values for each of \n",
    "    the hyperparamters and decide which combination gives the best result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator found by grid search:\n",
      "SVC(C=10, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.78475997035146072,\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "param_grid = {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'kernel': ['rbf', 'linear'],\n",
    "        'gamma': np.logspace(-3, 2, 20)}\n",
    "clf = GridSearchCV(svm.SVC(class_weight='balanced'), param_grid)\n",
    "clf = clf.fit(news_classifier.train_vectors, news_classifier.train_labels)\n",
    "print(\"Best estimator found by grid search:\")\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v. Learning curve\n",
    " - Learning curve depicts the behavior of model while increasing the number of train data points.\n",
    " http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title = 'Learning Curve : (SVM, linear kernel, $\\gamma=%.4f$)' % clf.best_estimator_.gamma\n",
    "estimator = svm.SVC(kernel='rbf', gamma=clf.best_estimator_.gamma, C = 10)\n",
    "plot_learning_curve(\n",
    "    estimator, title, news_classifier.train_vectors,\n",
    "    news_classifier.train_labels,\n",
    "    cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"learningcurve.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - In the figure, we can see that validation score increases with growing training instances, while the training score remains constant with a growing training sets. \n",
    " - Here, increasing training set size will obviously increase the validation score.\n",
    " - So, it can be concluded that 'rbf' kernel is a high variance estimator which over-fits our data. This high variance model can be improved by adding more training samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In my opinion, sentiment analysis can not be totally derived from concrete science as we are dealing with emotions and feelings of people. The best observation could be testing whether our classifier is good to predict correct output in unseen data set. And, at the same time, the selection of best classifier depends upon your need as well. For example, you are developing a model to classify patients cancer diagnosis report. Here, if you model happens to have higher False Positive value (means you model is saying that patient have cancer when they do not), it is a critical issue. In such case, you should look for a better model, try to use different classifier with different parameters for your test data, and figure out the best one.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
